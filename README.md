# ğŸš€ Qwen-Valencia - Sistema de IA Multimodal con Qwen + DeepSeek

## ğŸ¯ OBJETIVO

Sistema **100% LIMPIO** con Qwen + DeepSeek ejecutores en su **mÃ¡ximo esplendor**, sin contaminaciÃ³n descriptiva de ChatGPT/Claude.

**NO HAY BLOQUEOS. NO HAY DESCRIPCIONES. SOLO EJECUCIÃ“N REAL.**

Sistema auto-inteligente que selecciona automÃ¡ticamente el modelo apropiado segÃºn el tipo de tarea, con integraciÃ³n completa de Sandra IA 8.0 como orquestador maestro.

---

## ğŸ§  MODELOS

### OptimizaciÃ³n de Memoria RAM âš¡

Qwen-Valencia estÃ¡ optimizado con **7 modelos API de Groq** y **solo 2 modelos locales MUY LIGEROS** para reducir el uso de memoria RAM.

**Total de RAM local: ~1.7 GB** (vs ~11.7 GB anterior)

### Modelos API (Groq) - 7 modelos disponibles

#### Qwen (4 modelos)

- **Qwen 2.5 72B**: `qwen-2.5-72b-instruct` â­ MÃ¡s potente
- **Qwen 2.5 32B**: `qwen-2.5-32b-instruct` - Balanceado
- **Qwen 2.5 14B**: `qwen-2.5-14b-instruct` - RÃ¡pido
- **Qwen 2.5 7B**: `qwen-2.5-7b-instruct` - Ultra rÃ¡pido

#### DeepSeek (3 modelos)

- **DeepSeek R1 70B**: `deepseek-r1-distill-llama-70b` â­ Razonamiento profundo
- **DeepSeek R1 7B**: `deepseek-r1-distill-qwen-7b` - Razonamiento rÃ¡pido
- **DeepSeek R1 8B**: `deepseek-r1-distill-llama-8b` - Razonamiento balanceado

### Modelos Locales (Ollama) - 2 modelos MUY LIGEROS âš¡

- **Qwen 2.5 1.5B**: `qwen2.5:1.5b-instruct` - Conversacional ultra ligero (~986 MB RAM)
- **DeepSeek Coder 1.3B**: `deepseek-coder:1.3b` - Especializado en cÃ³digo ultra ligero (~776 MB RAM)

**Total: ~1.7 GB RAM** (optimizado para sistemas con poca memoria)

> âš ï¸ **Nota**: Los modelos pesados (`qwen2.5:7b`, `qwen2.5vl:3b`, `deepseek-coder:6.7b`, `deepseek-r1:7b`) fueron desinstalados para liberar ~10 GB de RAM. Para modelos potentes, usar API Groq.

---

## ğŸ¯ SISTEMA AUTO-INTELIGENTE

### SelecciÃ³n AutomÃ¡tica de Modelos

El sistema detecta automÃ¡ticamente el tipo de tarea y selecciona el modelo apropiado:

- **Razonamiento profundo** â†’ DeepSeek R1 (API) o DeepSeek Coder 1.3B (local)
- **CÃ³digo y programaciÃ³n** â†’ DeepSeek Coder (API o local)
- **Multimodal (imÃ¡genes)** â†’ Qwen (API o local)
- **Tareas generales** â†’ Qwen (API o local)
- **OrquestaciÃ³n compleja** â†’ Sandra IA 8.0 (orquestador maestro)

### DeepSeekExecutor - Sistema Auto-Inteligente

El `DeepSeekExecutor` incluye detecciÃ³n automÃ¡tica de tareas:

```javascript
// Detecta automÃ¡ticamente:
- taskType: 'reasoning' | 'code' | 'orchestration' | 'multimodal'
- Selecciona modelo apropiado segÃºn tipo
- Fallback hÃ­brido a Qwen si es necesario
```

**CaracterÃ­sticas:**

- âœ… DetecciÃ³n automÃ¡tica de tipo de tarea
- âœ… SelecciÃ³n inteligente de modelo (reasoning/code)
- âœ… Fallback hÃ­brido a Qwen para tareas multimodales
- âœ… Compatibilidad total con Qwen (sin supremacÃ­a)
- âœ… Circuit breakers y retry logic
- âœ… ValidaciÃ³n de parÃ¡metros
- âœ… Manejo de errores unificado

---

## ğŸš€ INSTALACIÃ“N

```bash
# 1. Clonar repo
git clone https://github.com/GUESTVALENCIA/Qwen-Valencia.git
cd Qwen-Valencia

# 2. Instalar dependencias
npm install

# 3. Configurar variables de entorno
# Crear archivo qwen-valencia.env con:
GROQ_API_KEY=tu_api_key_aqui

# 4. Instalar modelos Ollama (solo modelos ligeros)
ollama pull qwen2.5:1.5b-instruct      # Qwen ultra ligero (~986 MB)
ollama pull deepseek-coder:1.3b         # DeepSeek ultra ligero (~776 MB)

# 5. Limpiar modelos pesados (opcional)
node scripts/cleanup-ollama-models.js

# 6. Iniciar aplicaciÃ³n
npm start
```

---

## âš™ï¸ CONFIGURACIÃ“N

### Variables de Entorno (`qwen-valencia.env`)

```env
# Groq API (requerido para modelos API)
GROQ_API_KEY=tu_api_key_aqui

# Ollama (opcional, para modelos locales)
OLLAMA_BASE_URL=http://localhost:11434

# MCP Universal Server
MCP_PORT=6000
MCP_SECRET_KEY=tu_secret_key_aqui

# Modo por defecto
MODE=auto  # auto, groq, ollama
```

### Modelos Locales (Ollama)

**ConfiguraciÃ³n automÃ¡tica:**

- Qwen: `qwen2.5:1.5b-instruct` (auto-detectado)
- DeepSeek: `deepseek-coder:1.3b` (auto-detectado)

**Variables de entorno opcionales:**

```env
QWEN_MODEL_OLLAMA=qwen2.5:1.5b-instruct
DEEPSEEK_MODEL_OLLAMA_CODE=deepseek-coder:1.3b
DEEPSEEK_MODEL_OLLAMA_REASONING=deepseek-coder:1.3b
```

---

## ğŸ¯ CARACTERÃSTICAS

### Ejecutores

âœ… **QwenExecutor**: NÃºcleo ejecutor puro para Qwen
âœ… **DeepSeekExecutor**: NÃºcleo ejecutor puro con sistema auto-inteligente
âœ… **ModelRouter**: Routing inteligente entre modelos
âœ… **Sandra IA 8.0**: Orquestador maestro integrado

### Capacidades

âœ… **EjecuciÃ³n Real**: NO describe, EJECUTA
âœ… **Sin Bloqueos**: Sin webhooks de ChatGPT/Claude
âœ… **Multimodal**: Qwen procesa imÃ¡genes y texto
âœ… **Especializado**: DeepSeek para cÃ³digo y razonamiento profundo
âœ… **Sistema Auto**: SelecciÃ³n inteligente de modelos segÃºn tarea
âœ… **Flexible**: 7 modelos API + 2 modelos locales ultra ligeros
âœ… **Optimizado**: Solo ~1.7 GB RAM para modelos locales
âœ… **HÃ­brido**: Compatibilidad total Qwen + DeepSeek (sin supremacÃ­a)
âœ… **OrquestaciÃ³n**: IntegraciÃ³n con Sandra IA 8.0
âœ… **100% Limpio**: Sin contaminaciÃ³n descriptiva

---

## ğŸ“‹ ESTRUCTURA

```
Qwen-Valencia/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ qwen-executor.js          # NÃºcleo ejecutor Qwen
â”‚   â”‚   â””â”€â”€ deepseek-executor.js      # NÃºcleo ejecutor DeepSeek (auto-inteligente)
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â””â”€â”€ model-router.js           # Routing inteligente Qwen + DeepSeek + Sandra
â”‚   â”œâ”€â”€ mcp/
â”‚   â”‚   â”œâ”€â”€ mcp-universal.js          # Servidor MCP Universal
â”‚   â”‚   â”œâ”€â”€ ollama-mcp-server.js      # Servidor MCP Ollama
â”‚   â”‚   â”œâ”€â”€ groq-api-server.js        # Servidor MCP Groq
â”‚   â”‚   â””â”€â”€ sandra-ia-mcp-server.js   # Servidor MCP Sandra IA 8.0
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.js                   # Electron main process
â”‚       â”œâ”€â”€ preload.js                # IPC bridge
â”‚       â””â”€â”€ renderer/
â”‚           â”œâ”€â”€ index.html            # UI principal
â”‚           â””â”€â”€ components/
â”‚               â”œâ”€â”€ app.js            # LÃ³gica frontend
â”‚               â””â”€â”€ model-selector.js # Selector de modelos
â”œâ”€â”€ core/
â”‚   â””â”€â”€ sandra-core/                  # NÃºcleo de Sandra IA 8.0
â”‚       â”œâ”€â”€ orchestrator.js           # Orquestador maestro
â”‚       â”œâ”€â”€ decision-engine.js        # Motor de decisiÃ³n
â”‚       â””â”€â”€ model-invoker.js          # Invocador de modelos
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ cleanup-ollama-models.js       # Limpieza de modelos pesados
â”‚   â””â”€â”€ health-check.js               # VerificaciÃ³n de salud
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ models.json                   # ConfiguraciÃ³n de modelos
â”‚   â””â”€â”€ sandra-orchestrator.json      # ConfiguraciÃ³n Sandra IA
â”œâ”€â”€ qwen-valencia.env                 # Variables de entorno
â””â”€â”€ package.json
```

---

## ğŸ”¥ INTEGRACIÃ“N CON SANDRA IA 8.0

### Sandra IA como Orquestador Maestro

Sandra IA 8.0 estÃ¡ integrada como otro modelo disponible en el sistema:

- **Modelo**: `sandra-ia-8.0`
- **Puerto MCP**: `6004`
- **Capacidades**: OrquestaciÃ³n, multimodal, subagentes, razonamiento avanzado
- **Fallback**: Si Sandra IA no estÃ¡ disponible, usa Qwen como fallback

### SelecciÃ³n de Modelos

En la aplicaciÃ³n, puedes seleccionar entre:

- **Sandra IA 8.0**: Orquestador maestro con 117 subagentes
- **QWEN Valencia**: Sistema auto que selecciona Qwen o DeepSeek
- **Modelos individuales**: Qwen o DeepSeek especÃ­ficos

---

## ğŸš€ USO

### Ejemplo BÃ¡sico

```javascript
const ModelRouter = require('./src/orchestrator/model-router');

const router = new ModelRouter();

// El sistema auto-detecta el tipo de tarea y selecciona el modelo apropiado
const response = await router.route(
  'Analiza este cÃ³digo y sugiere mejoras',
  'text',
  [],
  { model: 'auto' } // Sistema auto-inteligente
);
```

### Ejemplo con DeepSeekExecutor

```javascript
const DeepSeekExecutor = require('./src/core/deepseek-executor');

const deepseek = new DeepSeekExecutor({
  groqApiKey: process.env.GROQ_API_KEY
});

// El sistema detecta automÃ¡ticamente que es cÃ³digo y usa deepseek-coder
const response = await deepseek.execute('Escribe una funciÃ³n Python para ordenar una lista');
```

### Ejemplo con Sandra IA

```javascript
const response = await router.route(
  'Orquesta una tarea compleja que requiere mÃºltiples modelos',
  'text',
  [],
  { model: 'sandra-ia-8.0' }
);
```

---

## ğŸ“Š COMPARATIVA DE MODELOS

### Modelos API (Groq) - 7 modelos

| Modelo          | Tokens | Velocidad | Uso Principal               |
| --------------- | ------ | --------- | --------------------------- |
| Qwen 2.5 72B    | 32K    | âš¡âš¡âš¡    | General, mÃ¡ximo rendimiento |
| Qwen 2.5 32B    | 32K    | âš¡âš¡      | General, balanceado         |
| Qwen 2.5 14B    | 32K    | âš¡âš¡      | General, rÃ¡pido             |
| Qwen 2.5 7B     | 32K    | âš¡âš¡âš¡    | General, ultra rÃ¡pido       |
| DeepSeek R1 70B | 8K     | âš¡âš¡âš¡    | Razonamiento profundo       |
| DeepSeek R1 7B  | 8K     | âš¡âš¡âš¡    | Razonamiento rÃ¡pido         |
| DeepSeek R1 8B  | 8K     | âš¡âš¡      | Razonamiento balanceado     |

### Modelos Locales (Ollama) - 2 modelos ultra ligeros

| Modelo              | Tokens | Velocidad | Memoria | Uso Principal              |
| ------------------- | ------ | --------- | ------- | -------------------------- |
| Qwen 2.5 1.5B       | 32K    | âš¡        | ~986 MB | General local ultra ligero |
| DeepSeek Coder 1.3B | 16K    | âš¡        | ~776 MB | CÃ³digo local ultra ligero  |

**Total**: 9 modelos (7 API + 2 locales ultra ligeros)

---

## ğŸ› ï¸ SCRIPTS DISPONIBLES

```bash
# Iniciar aplicaciÃ³n
npm start

# Limpiar modelos pesados de Ollama
node scripts/cleanup-ollama-models.js

# Verificar salud del sistema
npm run health

# Validar configuraciÃ³n
npm run validate

# Probar orquestador Sandra
npm run test
```

---

## ğŸ’¾ OPTIMIZACIÃ“N DE MEMORIA

### Antes de la OptimizaciÃ³n

- Modelos locales: ~11.7 GB RAM
- Modelos pesados instalados

### DespuÃ©s de la OptimizaciÃ³n

- Modelos locales: ~1.7 GB RAM
- Solo modelos ultra ligeros
- **LiberaciÃ³n: ~10 GB RAM**

### Recomendaciones

- **Sistemas con poca RAM (< 8GB)**: Usar solo modelos API (Groq)
- **Sistemas con RAM media (8-16GB)**: Usar modelos locales ligeros + API
- **Sistemas con mucha RAM (> 16GB)**: Pueden instalar modelos mÃ¡s pesados si lo desean

---

## ğŸ”„ FLUJO DE FUNCIONAMIENTO

### 1. RecepciÃ³n de Tarea

- Usuario envÃ­a tarea/pregunta
- Sistema auto-inteligente analiza la tarea
- Identifica tipo: reasoning, code, multimodal, orchestration

### 2. SelecciÃ³n de Modelo

- **Sistema Auto**: Selecciona Qwen o DeepSeek segÃºn tipo
- **DeepSeekExecutor**: Auto-detecta y selecciona modelo apropiado
- **Sandra IA**: Orquesta tareas complejas con mÃºltiples modelos

### 3. EjecuciÃ³n

- **API (Groq)**: RÃ¡pido, potente, sin uso de RAM local
- **Local (Ollama)**: Privado, ultra ligero, solo ~1.7 GB RAM

### 4. Fallback

- Si API falla â†’ Local
- Si DeepSeek no es apropiado â†’ Qwen
- Si Sandra IA no estÃ¡ disponible â†’ Qwen

### 5. Respuesta

- Sistema presenta resultado al usuario
- Registra mÃ©tricas de uso y Ã©xito

---

## ğŸ”¥ DIFERENCIAS CON OTROS SISTEMAS

| CaracterÃ­stica    | Otros Sistemas        | Qwen-Valencia                 |
| ----------------- | --------------------- | ----------------------------- |
| **NÃºcleo**        | Descriptivo (ChatGPT) | Ejecutor puro (Qwen/DeepSeek) |
| **Bloqueos**      | âœ… SÃ­ (webhooks)      | âŒ NO                         |
| **EjecuciÃ³n**     | Describe acciones     | Ejecuta acciones              |
| **Modelos**       | GPT/Claude/Gemini     | Qwen/DeepSeek                 |
| **Sistema Auto**  | âŒ No                 | âœ… SÃ­ (detecciÃ³n automÃ¡tica)  |
| **Memoria Local** | ~11+ GB               | ~1.7 GB                       |
| **OrquestaciÃ³n**  | âŒ No                 | âœ… Sandra IA 8.0              |
| **ContaminaciÃ³n** | âœ… SÃ­                 | âŒ NO                         |

---

## ğŸ“ LICENCIA

MIT - Libre para uso personal y comercial

---

## ğŸ†˜ SOPORTE

### Problemas Comunes

**Error: "Cannot find module '../core/deepseek-executor'"**

- SoluciÃ³n: Verificar que `src/core/deepseek-executor.js` existe

**Error: "GROQ_API_KEY no encontrada"**

- SoluciÃ³n: Crear archivo `qwen-valencia.env` con `GROQ_API_KEY=tu_key`

**Modelos locales no funcionan**

- SoluciÃ³n: Ejecutar `ollama pull qwen2.5:1.5b-instruct` y `ollama pull deepseek-coder:1.3b`

**Memoria RAM insuficiente**

- SoluciÃ³n: Usar solo modelos API (Groq), desinstalar modelos locales pesados

---

**Creado con â¤ï¸ para ejecuciÃ³n REAL sin bloqueos**

**VersiÃ³n**: 1.0.0  
**Ãšltima actualizaciÃ³n**: 2025-12-04  
**OptimizaciÃ³n**: 7 modelos API + 2 modelos locales ultra ligeros (~1.7 GB RAM)
