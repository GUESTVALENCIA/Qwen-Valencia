# üìã Modelos Configurados - Qwen-Valencia

## üéØ Resumen

Qwen-Valencia est√° optimizado con **6+ modelos API de Groq** y **solo 2 modelos locales ligeros** para optimizar el uso de memoria RAM. Los modelos est√°n configurados para usar **Qwen** y **DeepSeek** exclusivamente.

**Optimizaci√≥n de memoria**: Se redujeron los modelos locales pesados para mejorar el rendimiento en sistemas con poca RAM.

---

## üöÄ Modelos Qwen

### Modelos API (Groq) - 4 modelos disponibles

#### Qwen 2.5 72B Instruct (Groq API) ‚≠ê Recomendado
- **ID**: `qwen-2.5-72b-instruct`
- **Proveedor**: Groq
- **Tokens**: 32K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: General, razonamiento complejo, m√°ximo rendimiento
- **Modelo Groq**: `qwen2.5-72b-instruct`
- **Estado**: ‚úÖ Disponible v√≠a API

#### Qwen 2.5 32B Instruct (Groq API)
- **ID**: `qwen-2.5-32b-instruct`
- **Proveedor**: Groq
- **Tokens**: 32K
- **Velocidad**: ‚ö°‚ö° R√°pido
- **Uso**: General, balance perfecto entre potencia y velocidad
- **Modelo Groq**: `qwen2.5-32b-instruct`
- **Estado**: ‚úÖ Disponible v√≠a API

#### Qwen 2.5 14B Instruct (Groq API)
- **ID**: `qwen-2.5-14b-instruct`
- **Proveedor**: Groq
- **Tokens**: 32K
- **Velocidad**: ‚ö°‚ö° R√°pido
- **Uso**: General, tareas r√°pidas
- **Modelo Groq**: `qwen2.5-14b-instruct`
- **Estado**: ‚úÖ Disponible v√≠a API

#### Qwen 2.5 7B Instruct (Groq API)
- **ID**: `qwen-2.5-7b-instruct`
- **Proveedor**: Groq
- **Tokens**: 32K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: General, respuestas instant√°neas
- **Modelo Groq**: `qwen2.5-7b-instruct`
- **Estado**: ‚úÖ Disponible v√≠a API

### Modelos Locales (Ollama) - 1 modelo ligero

#### Qwen 2.5 7B Instruct (Ollama Local) ‚≠ê √önico modelo local Qwen
- **ID**: `qwen2.5:7b-instruct`
- **Proveedor**: Ollama
- **Tokens**: 32K
- **Velocidad**: üê¢ Medio (depende de GPU)
- **Uso**: General, privacidad total, modelo ligero
- **Modelo Ollama**: `qwen2.5:7b-instruct`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull qwen2.5:7b-instruct`
- **Memoria requerida**: ~4GB RAM

**Nota**: El modelo `qwen2.5vl:3b` fue eliminado para optimizar memoria. Para im√°genes, se usa Qwen est√°ndar.

---

## üß† Modelos DeepSeek

### Modelos API (Groq) - 3 modelos disponibles

#### DeepSeek R1 70B Distill Llama (Groq API) ‚≠ê Recomendado
- **ID**: `deepseek-r1-distill-llama-70b`
- **Proveedor**: Groq
- **Tokens**: 8K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: Razonamiento profundo, an√°lisis complejo, inferencia l√≥gica
- **Modelo Groq**: `deepseek-r1-distill-llama-70b`
- **Estado**: ‚úÖ Disponible v√≠a API

#### DeepSeek R1 7B Distill Qwen (Groq API)
- **ID**: `deepseek-r1-distill-qwen-7b`
- **Proveedor**: Groq
- **Tokens**: 8K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: Razonamiento r√°pido y eficiente
- **Modelo Groq**: `deepseek-r1-distill-qwen-7b`
- **Estado**: ‚úÖ Disponible v√≠a API

#### DeepSeek R1 8B Distill Llama (Groq API)
- **ID**: `deepseek-r1-distill-llama-8b`
- **Proveedor**: Groq
- **Tokens**: 8K
- **Velocidad**: ‚ö°‚ö° R√°pido
- **Uso**: Razonamiento balanceado
- **Modelo Groq**: `deepseek-r1-distill-llama-8b`
- **Estado**: ‚úÖ Disponible v√≠a API

### Modelos Locales (Ollama) - 1 modelo ligero

#### DeepSeek Coder 6.7B (Ollama Local) ‚≠ê √önico modelo local DeepSeek
- **ID**: `deepseek-coder:6.7b`
- **Proveedor**: Ollama
- **Tokens**: 16K
- **Velocidad**: ‚ö° R√°pido
- **Uso**: Especializado en c√≥digo, programaci√≥n, debugging
- **Modelo Ollama**: `deepseek-coder:6.7b`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull deepseek-coder:6.7b`
- **Memoria requerida**: ~3GB RAM

**Nota**: El modelo `deepseek-r1:7b` fue eliminado para optimizar memoria.

---

## üîÑ Modo Auto

El sistema incluye un **Modo Auto** optimizado que selecciona autom√°ticamente el modelo m√°s apropiado seg√∫n:
- **Tipo de tarea**: 
  - C√≥digo ‚Üí DeepSeek Coder (local) o DeepSeek R1 (API)
  - Razonamiento ‚Üí DeepSeek R1 (API) o DeepSeek Coder (local)
  - General ‚Üí Qwen 72B/32B (API) o Qwen 7B (local)
- **Presencia de im√°genes**: Qwen est√°ndar (local o API seg√∫n configuraci√≥n)
- **Toggle API**: 
  - API activado (por defecto) ‚Üí Prioriza modelos Groq API
  - API desactivado ‚Üí Usa solo modelos locales ligeros

**Optimizaci√≥n**: El modo Auto prioriza modelos API cuando est√°n disponibles para mejor rendimiento y menor uso de memoria.

---

## üìä Comparativa de Modelos

### Modelos API (Groq) - 7 modelos

| Modelo | Tokens | Velocidad | Uso Principal |
|--------|--------|-----------|---------------|
| Qwen 2.5 72B | 32K | ‚ö°‚ö°‚ö° | General, m√°ximo rendimiento |
| Qwen 2.5 32B | 32K | ‚ö°‚ö° | General, balanceado |
| Qwen 2.5 14B | 32K | ‚ö°‚ö° | General, r√°pido |
| Qwen 2.5 7B | 32K | ‚ö°‚ö°‚ö° | General, ultra r√°pido |
| DeepSeek R1 70B | 8K | ‚ö°‚ö°‚ö° | Razonamiento profundo |
| DeepSeek R1 7B | 8K | ‚ö°‚ö°‚ö° | Razonamiento r√°pido |
| DeepSeek R1 8B | 8K | ‚ö°‚ö° | Razonamiento balanceado |

### Modelos Locales (Ollama) - 2 modelos ligeros

| Modelo | Tokens | Velocidad | Memoria | Uso Principal |
|--------|--------|-----------|---------|---------------|
| Qwen 2.5 7B | 32K | üê¢ | ~4GB | General local |
| DeepSeek Coder 6.7B | 16K | ‚ö° | ~3GB | C√≥digo, programaci√≥n |

**Total**: 9 modelos (7 API + 2 locales)

---

## üõ†Ô∏è Instalaci√≥n de Modelos Ollama

Para usar modelos locales, instala Ollama y descarga **solo los 2 modelos ligeros**:

```bash
# Instalar Ollama (si no est√° instalado)
# Descargar desde: https://ollama.ai

# Modelos locales ligeros (solo 2)
ollama pull qwen2.5:7b-instruct      # Qwen ligero
ollama pull deepseek-coder:6.7b      # DeepSeek Coder ligero
```

**Nota**: Los modelos pesados (`qwen2.5vl:3b`, `deepseek-r1:7b`) fueron eliminados para optimizar memoria. Se recomienda usar modelos API para mejor rendimiento.

---

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

```env
# Groq API (para modelos online)
GROQ_API_KEY=tu_api_key_aqui

# Ollama (para modelos locales)
OLLAMA_BASE_URL=http://localhost:11434

# Modo por defecto
MODE=auto  # auto, groq, ollama
```

### Selecci√≥n de Modelo

1. **Manual**: Usa el selector de modelos en la UI
2. **Auto**: Activa el modo auto para selecci√≥n autom√°tica
3. **Multi-modelo**: Selecciona m√∫ltiples modelos para comparar respuestas

---

## üíæ Optimizaci√≥n de Memoria

El sistema incluye verificaci√≥n autom√°tica de memoria RAM:
- **Advertencia**: Si hay menos de 4GB RAM disponible, se muestra advertencia al usar modelos locales
- **Recomendaci√≥n**: Usar modelos API cuando hay poca memoria disponible
- **Modelos locales**: Requieren al menos 4-6GB RAM libre para funcionar correctamente

## üìù Notas

- **Modelos API**: No requieren descarga ni GPU local, solo conexi√≥n a internet y API key v√°lida
- **Modelos locales**: Requieren instalaci√≥n con `ollama pull` y GPU recomendada (4GB+ RAM)
- **Modo Auto**: Prioriza modelos API cuando est√°n disponibles para mejor rendimiento
- **Memoria**: Se redujeron modelos locales pesados para optimizar uso de RAM
- Los modelos se pueden cambiar en tiempo real desde la UI

## üóëÔ∏è Modelos Eliminados

Los siguientes modelos fueron eliminados para optimizar memoria:
- `qwen2.5vl:3b` - Modelo VL pesado (reemplazado por Qwen est√°ndar)
- `deepseek-r1:7b` - Modelo R1 pesado (reemplazado por DeepSeek Coder ligero)

**Raz√≥n**: Estos modelos requer√≠an mucha memoria RAM y fueron reemplazados por modelos m√°s ligeros o equivalentes en API.

---

**√öltima actualizaci√≥n**: 2025-12-03
**Versi√≥n**: Qwen-Valencia v1.0.0
**Optimizaci√≥n**: 6+ modelos API, 2 modelos locales ligeros
