# üìã Modelos Configurados - Qwen-Valencia

## üéØ Resumen

Qwen-Valencia est√° configurado para usar modelos **Qwen** y **DeepSeek** exclusivamente, tanto v√≠a **Groq API** (online, r√°pido) como **Ollama** (local, privado).

---

## üöÄ Modelos Qwen

### Qwen 2.5 72B Instruct (Groq API)
- **ID**: `qwen-2.5-72b-instruct`
- **Proveedor**: Groq
- **Tokens**: 8K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: General, razonamiento complejo, ejecuci√≥n de c√≥digo
- **Modelo Groq**: `qwen2.5-72b-instruct`
- **Estado**: ‚úÖ Disponible v√≠a API

### Qwen 2.5 7B Instruct (Ollama Local)
- **ID**: `qwen2.5:7b-instruct`
- **Proveedor**: Ollama
- **Tokens**: 32K
- **Velocidad**: üê¢ Medio (depende de GPU)
- **Uso**: General, privacidad total
- **Modelo Ollama**: `qwen2.5:7b-instruct`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull qwen2.5:7b-instruct`

### Qwen 2.5 VL 3B (Ollama Local - Multimodal)
- **ID**: `qwen2.5vl:3b`
- **Proveedor**: Ollama
- **Tokens**: 32K
- **Velocidad**: üê¢ Medio
- **Uso**: Procesamiento de im√°genes, visi√≥n
- **Modelo Ollama**: `qwen2.5vl:3b`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull qwen2.5vl:3b`

### Qwen 2.5 3B Instruct (Ollama Local)
- **ID**: `qwen2.5:3b-instruct`
- **Proveedor**: Ollama
- **Tokens**: 32K
- **Velocidad**: ‚ö° R√°pido (modelo peque√±o)
- **Uso**: Tareas r√°pidas, respuestas cortas
- **Modelo Ollama**: `qwen2.5:3b-instruct`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull qwen2.5:3b-instruct`

---

## üß† Modelos DeepSeek

### DeepSeek R1 70B Distill Llama (Groq API)
- **ID**: `deepseek-r1-distill-llama-70b`
- **Proveedor**: Groq
- **Tokens**: 8K
- **Velocidad**: ‚ö°‚ö°‚ö° Ultra r√°pido
- **Uso**: Razonamiento profundo, an√°lisis complejo, inferencia l√≥gica
- **Modelo Groq**: `deepseek-r1-distill-llama-70b`
- **Estado**: ‚úÖ Disponible v√≠a API

### DeepSeek R1 7B (Ollama Local)
- **ID**: `deepseek-r1:7b`
- **Proveedor**: Ollama
- **Tokens**: 32K
- **Velocidad**: üê¢ Medio
- **Uso**: Razonamiento profundo local, an√°lisis complejo
- **Modelo Ollama**: `deepseek-r1:7b`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull deepseek-r1:7b`

### DeepSeek Coder 6.7B (Ollama Local)
- **ID**: `deepseek-coder:6.7b`
- **Proveedor**: Ollama
- **Tokens**: 16K
- **Velocidad**: ‚ö° R√°pido
- **Uso**: Especializado en c√≥digo, programaci√≥n, debugging
- **Modelo Ollama**: `deepseek-coder:6.7b`
- **Estado**: ‚ö†Ô∏è Requiere instalaci√≥n: `ollama pull deepseek-coder:6.7b`

---

## üîÑ Modo Auto

El sistema incluye un **Modo Auto** que selecciona autom√°ticamente el modelo m√°s apropiado seg√∫n:
- **Tipo de tarea**: C√≥digo ‚Üí DeepSeek, General ‚Üí Qwen
- **Presencia de im√°genes**: Si hay im√°genes ‚Üí Qwen VL
- **Toggle API**: API activado ‚Üí Groq, API desactivado ‚Üí Ollama

---

## üìä Comparativa de Modelos

| Modelo | Proveedor | Tokens | Velocidad | Uso Principal |
|--------|-----------|--------|-----------|---------------|
| Qwen 2.5 72B | Groq | 8K | ‚ö°‚ö°‚ö° | General, razonamiento |
| Qwen 2.5 7B | Ollama | 32K | üê¢ | General local |
| Qwen 2.5 VL 3B | Ollama | 32K | üê¢ | Im√°genes, visi√≥n |
| Qwen 2.5 3B | Ollama | 32K | ‚ö° | Tareas r√°pidas |
| DeepSeek R1 70B | Groq | 8K | ‚ö°‚ö°‚ö° | Razonamiento profundo |
| DeepSeek R1 7B | Ollama | 32K | üê¢ | Razonamiento local |
| DeepSeek Coder 6.7B | Ollama | 16K | ‚ö° | C√≥digo, programaci√≥n |

---

## üõ†Ô∏è Instalaci√≥n de Modelos Ollama

Para usar modelos locales, instala Ollama y descarga los modelos:

```bash
# Instalar Ollama (si no est√° instalado)
# Descargar desde: https://ollama.ai

# Modelos Qwen
ollama pull qwen2.5:7b-instruct
ollama pull qwen2.5vl:3b
ollama pull qwen2.5:3b-instruct

# Modelos DeepSeek
ollama pull deepseek-r1:7b
ollama pull deepseek-coder:6.7b
```

---

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

```env
# Groq API (para modelos online)
GROQ_API_KEY=tu_api_key_aqui

# Ollama (para modelos locales)
OLLAMA_BASE_URL=http://localhost:11434

# Modo por defecto
MODE=auto  # auto, groq, ollama
```

### Selecci√≥n de Modelo

1. **Manual**: Usa el selector de modelos en la UI
2. **Auto**: Activa el modo auto para selecci√≥n autom√°tica
3. **Multi-modelo**: Selecciona m√∫ltiples modelos para comparar respuestas

---

## üìù Notas

- Los modelos Groq requieren conexi√≥n a internet y API key v√°lida
- Los modelos Ollama requieren instalaci√≥n local y GPU recomendada
- El modo auto selecciona el mejor modelo seg√∫n la tarea
- Los modelos se pueden cambiar en tiempo real desde la UI

---

**√öltima actualizaci√≥n**: 2025-12-03
**Versi√≥n**: Qwen-Valencia v1.0.0
